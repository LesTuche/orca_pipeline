#!/bin/bash
readonly ARGS=("$@")

# your orca input file MUST provide %pal and %maxcore blocks for this script to work!
# you also must be on the new software stack

function get_args () {
    # Set up default parameters
    scr=8000 # scratch space in MB (per Node, not per core as in LSF)
    walltime=24 # wall time in hours
    orcaversion=601 # default ORCA version
    xtbversion=6.7
    output=slurm-%j.out
    mail=True
    # Parse the arguments
    local OPTIND OPTARG flag w r v x o m
    while getopts ':w:r:v:x:o:m:' flag; do
        case $flag in
            w) walltime=${OPTARG};;
            r) scr=${OPTARG};;
            v) orcaversion=${OPTARG};;
            x) xtbversion=${OPTARG};;
            o) output=${OPTARG};;
            m) mail=${OPTARG};; 
            *)
              usage
              exit
              ;;
        esac
    done
    file=${@:$OPTIND:1}

    if [ "$mail" == True  ]; then
        mail="#SBATCH --mail-type=END,FAIL"
    else
        mail=""
    fi

}

function set_vars_for_orca_version (){
    case $1 in
        503)
            modules="stack/2024-06 openmpi/4.1.6 orca/5.0.3";;
        504)
            modules="stack/2024-06 openmpi/4.1.6 orca/5.0.4";;
        600)
            modules="stack/2024-06 openmpi/4.1.6 orca/6.0.0";;
        601) 
            modules="stack/2024-06 openmpi/4.1.6 orca/6.0.1";;
        *)
            usage
            exit
            ;;
    esac
}

function usage () {
    echo "  ssubo [-r scratchspace_in_MB] [-w walltime_in_h] [-v ORCA_version] filename"
    echo "  (default parameters are -r 8000, -w 24 and -v 504)"
    echo "  you must be using the new LMOD software stack"
    echo "  your input files must define %pal and %maxcore blocks"
    echo "  your input file must have a newline at the end"
}

function main () {
    if [[ ${#ARGS[@]} -eq 0 ]]; then
        usage
        exit
    fi

    get_args "${ARGS[@]}"
    fpath=$(readlink -f "$file")
    fdir=$(dirname "$fpath")
    filename=$(basename "$fpath")
    name=${filename/.inp}

#   dos2unix "$fpath" # converting file to unix from dos

    nproc=$(grep -i "%pal" "$fpath" | awk '{print $3}' | tac | tail -n1) # get number of processors from Orca input
    memo=$(grep -i "%maxcore" "$fpath" | awk '{print $2}' | tac | tail -n1) # get memory per processor from Orca input
    memo=$((memo * 125 / 100)) # multiply by some overhead as Orca always uses more memory than you request

    set_vars_for_orca_version "$orcaversion"

    module purge
    if [ -n "$xtbversion" ] ; then
        module load xtb/"$xtbversion"
        echo "$xtbversion"
        XTBEXE="$(which xtb)"
        export XTBEXE
    fi
    echo "$modules"
    module load $modules    # Removed quotes to load modules separately
    module list

    orcaexec=$(which orca)
    if [ -n "$orcaexec" ]; then
        echo "orca executable found: $orcaexec"
    else
        echo "orca executable not found! Aborting!"
        exit 1
    fi

## 
#SBATCH --constraint=ibfabric6  # constraint jobs to only run on eu1 nodes

    export RSH_COMMAND="ssh" # Required to run multi-process numerical calculations on multiple nodes for instance for jobs with more than 24 cores
    export OMP_NUM_THREADS=1 # Orca uses MPI to parallelize a job and not OpenMP (1 thread per process), Orca 3.0.3 has problems when set to $nproc
    export OMPI_MCA_btl=self,tcp,vader # Removes messages about OpenMPI network interface in the output files

    sbatch <<-script
#!/bin/bash

#SBATCH -J ${fpath//"/cluster/scratch/$USER/"}
#SBATCH --mem-per-cpu=$memo
#SBATCH -n $nproc
#SBATCH --constraint=ibfabric6 
#SBATCH --tmp=$scr
#SBATCH --time=$walltime:00:00
#SBATCH --output=$output
$mail
sacct -j "\$SLURM_JOB_ID" -B

toinp="\$TMPDIR/$filename" # defining the path of the input file on local scratch
cp "$fpath" "\$toinp" # copying the input file to local scratch
cp "$fdir"/*.xyz "\$TMPDIR/" # copy forth coordinate files potentially used as input
cp "$fdir"/*.gbw "\$TMPDIR/" # copy forth orbital files potentially used as input
cp "$fdir"/*.bas "\$TMPDIR/" # copy forth basis set files potentially used as input
cp "$fdir"/*.hess "\$TMPDIR/" # copy hess files 
cp "$fdir"/*.pc "\$TMPDIR/" # copy pointcharges file 

cd "\$TMPDIR/" # Orca needs to be executed from the same folder of the input so that the temporary files are also there
(
    echo "Current Directory: \$PWD"
    while [ ! -f fin ]; do
        cp -u "\$TMPDIR"/*.out "$fdir/" # synchronize output file while calculation still running
        cp -u "\$TMPDIR"/*.xyz "$fdir/"
        cp -u "\$TMPDIR/${name}.gbw" "$fdir/"
        cp -u "\$TMPDIR"/*.interp "$fdir/"
        cp -u "\$TMPDIR"/*.log "$fdir/"
        sleep 1m # only copy back files every minute
    done
) &

(
    "$orcaexec" "\$toinp" > "${fpath/.inp/.out}" 2> "${fpath/.inp/.err}" || touch ERROR
    touch "\$TMPDIR/fin" # creates a file called fin signaling that the Orca job finished
) &

wait
rm "\$TMPDIR/fin" # delete fin file as it is not needed
rm "\$TMPDIR"/*.tmp* 2> /dev/null # delete all temp files before copying back
rm "\$TMPDIR"/*.bas*
cp -u "\$TMPDIR/"* "$fdir/" # copying back all files after finishing the calculation excluding folders with content

myjobs -j \$SLURM_JOB_ID

script
    echo "Orca Version:" $orcaversion
    echo "Cores:" "$nproc"
    echo "Time (hours):" "$walltime:00"
    echo "Memory (MB per core):" "$memo"
    echo "Scratch space (MB per node):" "$scr"
    echo "Input file:" "$fpath"
}

main
